\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Choose a title for your submission
\title{NLU19 Project 2: Story Cloze Task}


\author{Amr Eltelt \qquad Gagan Narula \qquad Nithya Shetty}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% We do not requrire you to write an abstract. Still, if you feel like it, please do so.
%\begin{abstract}
%\end{abstract}


\section{Introduction}
One of the hot topics in natural language understanding is in the field of research of text summarization and logical induction. In this work we try to address the Story Cloze task. Given a story of 4 sentences the algorithm needs to choose one of two ending as correlated to the text.
From the work done in several papers, it is clear that a good score can be achieved when training on the validation set. This approach achieved the highest benchmark score of 75.2. Even with this achievement, this approach is criticized as not being generalized to any corpus of stories. The current focus in this research filed is to achieve good results with a generalized algorithm. In this work we try to explore one particular paper work and implement the method with some modification. Also, we try to find a simpler approach for this task.
\section{Methodology}
Our approach is inspired by the Roemmele et al. In the paper different methods were applied. However the pipeline with best results comes from applying random negative ending to the training set, using skip-thoughts method for sentence embedding and finally applying RNN model.
Our pipeline is similar to that, however with a simpler approach.
In the training data, the second wrong ending was randomly selected from the training set. Therefore, all the data set have the same structure. 4 sentences + 2 ending + correct answer (0,1). These files are stored as CSV files.
The features selected from each dataset for our setup is the last 2 sentences in the story + the 2 ending and the labels are the correct answer (0 or 1)
We used pre-trained Skip-thoughts model from paper and implementation of Ryankiros (which is also used in the Roemmele methode) for sentence embedding. It is applied on selected features using both the uni-skip and bi-skip concatenated as this provides better results according to paper. 
\section{Model}
The model selected is a simple feed forward neural network for binary classification to predict the correct ending. The DNNClassifier model in the tensorflow was used. The network is designed with 3 layers with Relu activation 3200, 1600, and 800 units and softmax at the end. Dropout is 0.3.
The skip-thoughts embedding provides uni-skip and bi-skip vector with dimension 4800. Therefore, the total input features result into 4*4800. Also, experimented using only the 2 ending. Therefore, total input is 2*4800.
The default estimator loss function is used and Adam optimizer.
\section{Experiments}
The model was trained once with validation set and once with training set. Afterwards we tested on the test set.

Training on validation set: Accuracy= 66.6%

Training on training set: Accuracy= 58.6%

\section{Conclusion}
We here try to implement a Roemmele method with feedforward NN instead of GRU RNN model. Skip-thoughts was applied along with random negative ending. The results achieved is considerably poor compared to achieved benchmark in test summary papaer. Future work is required to improve model and experiment with different models. Example LSTM.
\end{document}
