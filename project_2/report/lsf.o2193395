Sender: LSF System <lsfadmin@lo-s4-018>
Subject: Job 2193395: <python main_train4.py> in cluster <leonhard> Done

Job <python main_train4.py> was submitted from host <lo-login-02> by user <aeltelt> in cluster <leonhard> at Tue Jun 11 22:58:05 2019
Job was executed on host(s) <4*lo-s4-018>, in queue <gpu.4h>, as user <aeltelt> in cluster <leonhard> at Tue Jun 11 22:58:30 2019
</cluster/home/aeltelt> was used as the home directory.
</cluster/scratch/aeltelt/nlu> was used as the working directory.
Started at Tue Jun 11 22:58:30 2019
Terminated at Tue Jun 11 22:59:26 2019
Results reported at Tue Jun 11 22:59:26 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main_train4.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43.46 sec.
    Max Memory :                                 2893 MB
    Average Memory :                             2083.25 MB
    Total Requested Memory :                     32768.00 MB
    Delta Memory :                               29875.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                30
    Run time :                                   78 sec.
    Turnaround time :                            81 sec.

The output (if any) follows:

WARNING:tensorflow:From main_train4.py:120: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

2019-06-11 22:58:47.665692: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-11 22:58:47.912385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2019-06-11 22:58:47.912426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-06-11 22:58:49.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-11 22:58:49.282649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-06-11 22:58:49.282659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-06-11 22:58:49.283601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10421 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)

Val and Tests sets Loaded
Epoch: 0001 cost= 14992.379596579
Epoch: 0002 cost= 3726.310689992
Epoch: 0003 cost= 1250.831540864
Epoch: 0004 cost= 511.848945486
Epoch: 0005 cost= 597.240092573
Epoch: 0006 cost= 970.358656390
Epoch: 0007 cost= 715.875724784
Epoch: 0008 cost= 1180.771358358
Epoch: 0009 cost= 2465.075741406
Epoch: 0010 cost= 2335.767983733
Epoch: 0011 cost= 2789.103392519
Epoch: 0012 cost= 640.203696810
Epoch: 0013 cost= 4.638150972
Epoch: 0014 cost= 0.000000000
Epoch: 0015 cost= 0.443244375
Epoch: 0016 cost= 0.000000000
Epoch: 0017 cost= 0.000000000
Epoch: 0018 cost= 0.000000000
Epoch: 0019 cost= 0.000000000
Epoch: 0020 cost= 0.000000000
Epoch: 0021 cost= 0.000000000
Epoch: 0022 cost= 0.000000000
Epoch: 0023 cost= 0.000000000
Epoch: 0024 cost= 0.000000000
Epoch: 0025 cost= 0.000000000
Epoch: 0026 cost= 0.000000000
Epoch: 0027 cost= 0.000000000
Epoch: 0028 cost= 0.000000000
Epoch: 0029 cost= 0.000000000
Epoch: 0030 cost= 0.000000000
Epoch: 0031 cost= 0.000000000
Epoch: 0032 cost= 0.000000000
Epoch: 0033 cost= 0.000000000
Epoch: 0034 cost= 0.000000000
Epoch: 0035 cost= 0.000000000
Epoch: 0036 cost= 0.000000000
Epoch: 0037 cost= 0.000000000
Epoch: 0038 cost= 0.000000000
Epoch: 0039 cost= 0.000000000
Epoch: 0040 cost= 0.000000000
Epoch: 0041 cost= 0.000000000
Epoch: 0042 cost= 0.000000000
Epoch: 0043 cost= 0.000000000
Epoch: 0044 cost= 0.000000000
Epoch: 0045 cost= 0.000000000
Epoch: 0046 cost= 0.000000000
Epoch: 0047 cost= 0.000000000
Epoch: 0048 cost= 0.000000000
Epoch: 0049 cost= 0.000000000
Epoch: 0050 cost= 0.000000000
Epoch: 0051 cost= 0.000000000
Epoch: 0052 cost= 0.000000000
Epoch: 0053 cost= 0.000000000
Epoch: 0054 cost= 0.000000000
Epoch: 0055 cost= 0.000000000
Epoch: 0056 cost= 0.000000000
Epoch: 0057 cost= 0.000000000
Epoch: 0058 cost= 0.000000000
Epoch: 0059 cost= 0.000000000
Epoch: 0060 cost= 0.000000000
Epoch: 0061 cost= 0.000000000
Epoch: 0062 cost= 0.000000000
Epoch: 0063 cost= 0.000000000
Epoch: 0064 cost= 0.000000000
Epoch: 0065 cost= 0.000000000
Epoch: 0066 cost= 0.000000000
Epoch: 0067 cost= 0.000000000
Epoch: 0068 cost= 0.000000000
Epoch: 0069 cost= 0.000000000
Epoch: 0070 cost= 0.000000000
Epoch: 0071 cost= 0.000000000
Epoch: 0072 cost= 0.000000000
Epoch: 0073 cost= 0.000000000
Epoch: 0074 cost= 0.000000000
Epoch: 0075 cost= 0.000000000
Epoch: 0076 cost= 0.000000000
Epoch: 0077 cost= 0.000000000
Epoch: 0078 cost= 0.000000000
Epoch: 0079 cost= 0.000000000
Epoch: 0080 cost= 0.000000000
Epoch: 0081 cost= 0.000000000
Epoch: 0082 cost= 0.000000000
Epoch: 0083 cost= 0.000000000
Epoch: 0084 cost= 0.000000000
Epoch: 0085 cost= 0.000000000
Epoch: 0086 cost= 0.000000000
Epoch: 0087 cost= 0.000000000
Epoch: 0088 cost= 0.000000000
Epoch: 0089 cost= 0.000000000
Epoch: 0090 cost= 0.000000000
Epoch: 0091 cost= 0.000000000
Epoch: 0092 cost= 0.000000000
Epoch: 0093 cost= 0.000000000
Epoch: 0094 cost= 0.000000000
Epoch: 0095 cost= 0.000000000
Epoch: 0096 cost= 0.000000000
Epoch: 0097 cost= 0.000000000
Epoch: 0098 cost= 0.000000000
Epoch: 0099 cost= 0.000000000
Epoch: 0100 cost= 0.000000000
Optimization Finished!
Accuracy: 0.6664885

Predicted endings saved
